"""
ì±„ìš©ê³µê³  ìë™ í¬ë¡¤ë§ ëª¨ë“ˆ.
Google News RSS + ì‚¬ëŒì¸/ì¡ì½”ë¦¬ì•„ ê²€ìƒ‰ìœ¼ë¡œ IBKìºí”¼íƒˆ, ì‚°ì€ìºí”¼íƒˆ ì±„ìš©ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from urllib.parse import quote
import re
import json
import os

# í¬ë¡¤ë§ ëŒ€ìƒ íšŒì‚¬ ì„¤ì •
COMPANIES = {
    "IBK": {
        "name": "IBKìºí”¼íƒˆ",
        "aliases": ["IBKìºí”¼íƒˆ", "IBK Capital", "ì•„ì´ë¹„ì¼€ì´ìºí”¼íƒˆ"],
        "career_url": "https://ibkcapital.co.kr/recruit",
        "saramin_keyword": "IBKìºí”¼íƒˆ",
    },
    "KDB": {
        "name": "ì‚°ì€ìºí”¼íƒˆ",
        "aliases": ["ì‚°ì€ìºí”¼íƒˆ", "KDBìºí”¼íƒˆ", "KDB Capital"],
        "career_url": "https://www.kdbcapital.co.kr",
        "saramin_keyword": "ì‚°ì€ìºí”¼íƒˆ",
    }
}

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

RECRUITMENT_KEYWORDS = ['ì±„ìš©', 'ê³µì±„', 'ì‹ ì…', 'ì¸í„´', 'ëª¨ì§‘', 'ì„ ë°œ', 'ê²½ë ¥', 'ìˆ˜ì‹œì±„ìš©', 'ì±„ìš©í˜•']
STORAGE_FILE = "recruitment_data.json"


def _load_stored_recruitment():
    """ì €ì¥ëœ ì±„ìš© ë°ì´í„° ë¡œë“œ."""
    if os.path.exists(STORAGE_FILE):
        try:
            with open(STORAGE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            pass
    return {"IBK": [], "KDB": [], "_last_checked": ""}


def _save_recruitment(data):
    """ì±„ìš© ë°ì´í„° ì €ì¥."""
    try:
        with open(STORAGE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def fetch_recruitment_from_news(company_key):
    """Google News RSSë¡œ ì±„ìš© ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘."""
    config = COMPANIES.get(company_key)
    if not config:
        return []

    items = []
    queries = [
        f'"{config["name"]}" (ì±„ìš© OR ê³µì±„ OR ëª¨ì§‘ OR ì¸í„´)',
        f'"{config["name"]}" (ì‹ ì… OR ê²½ë ¥ OR ìˆ˜ì‹œì±„ìš©)',
    ]

    for query in queries:
        try:
            encoded_query = quote(f'{query} when:90d')
            rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
            feed = feedparser.parse(rss_url)

            for entry in feed.entries:
                if len(items) >= 10:
                    break

                title = entry.title
                # ì±„ìš© ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸
                if not any(kw in title for kw in RECRUITMENT_KEYWORDS):
                    continue

                # ì¤‘ë³µ ì²´í¬
                if any(item['title'] == title for item in items):
                    continue

                items.append({
                    'title': title,
                    'link': entry.link,
                    'published': entry.published,
                    'source': 'google_news',
                    'company_key': company_key,
                })
        except Exception:
            continue

    return items


def fetch_recruitment_from_saramin(company_key):
    """ì‚¬ëŒì¸ì—ì„œ ì±„ìš©ê³µê³  ê²€ìƒ‰."""
    config = COMPANIES.get(company_key)
    if not config:
        return []

    items = []
    try:
        search_url = f"https://www.saramin.co.kr/zf_user/search?searchType=search&searchword={quote(config['saramin_keyword'])}&recruitSort=relation"
        response = requests.get(search_url, headers=HEADERS, timeout=10)

        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        # ì±„ìš©ê³µê³  ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
        job_cards = soup.select('.item_recruit') or soup.select('.list_body .list_item')

        for card in job_cards[:5]:
            try:
                title_elem = card.select_one('.job_tit a') or card.select_one('.str_tit a')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                link = title_elem.get('href', '')
                if link and not link.startswith('http'):
                    link = f"https://www.saramin.co.kr{link}"

                # íšŒì‚¬ëª… í™•ì¸
                company_elem = card.select_one('.corp_name a') or card.select_one('.company_nm a')
                company_name = company_elem.get_text(strip=True) if company_elem else ""

                # í•´ë‹¹ íšŒì‚¬ ê³µê³ ì¸ì§€ í™•ì¸
                if not any(alias in company_name for alias in config['aliases']):
                    if not any(alias in title for alias in config['aliases']):
                        continue

                # ë§ˆê°ì¼ ì¶”ì¶œ
                deadline_elem = card.select_one('.job_date .date') or card.select_one('.date')
                deadline = deadline_elem.get_text(strip=True) if deadline_elem else ""

                # ì¡°ê±´ ì¶”ì¶œ
                conditions_elem = card.select_one('.job_condition')
                conditions = conditions_elem.get_text(strip=True, separator=' | ') if conditions_elem else ""

                items.append({
                    'title': f"[ì‚¬ëŒì¸] {title}",
                    'link': link,
                    'published': datetime.now().strftime("%Y-%m-%d"),
                    'deadline': deadline,
                    'conditions': conditions,
                    'source': 'saramin',
                    'company_key': company_key,
                })
            except Exception:
                continue

    except Exception:
        pass

    return items


def fetch_recruitment_from_jobkorea(company_key):
    """ì¡ì½”ë¦¬ì•„ì—ì„œ ì±„ìš©ê³µê³  ê²€ìƒ‰."""
    config = COMPANIES.get(company_key)
    if not config:
        return []

    items = []
    try:
        search_url = f"https://www.jobkorea.co.kr/Search/?stext={quote(config['name'])}&tabType=recruit"
        response = requests.get(search_url, headers=HEADERS, timeout=10)

        if response.status_code != 200:
            return []

        soup = BeautifulSoup(response.text, 'html.parser')

        job_items = soup.select('.list-default .list-post') or soup.select('.recruit-info')

        for item in job_items[:5]:
            try:
                title_elem = item.select_one('.title a') or item.select_one('.post-list-info a')
                if not title_elem:
                    continue

                title = title_elem.get_text(strip=True)
                link = title_elem.get('href', '')
                if link and not link.startswith('http'):
                    link = f"https://www.jobkorea.co.kr{link}"

                # íšŒì‚¬ëª… í™•ì¸
                company_elem = item.select_one('.name a') or item.select_one('.corp-name a')
                company_name = company_elem.get_text(strip=True) if company_elem else ""

                if not any(alias in company_name for alias in config['aliases']):
                    if not any(alias in title for alias in config['aliases']):
                        continue

                items.append({
                    'title': f"[ì¡ì½”ë¦¬ì•„] {title}",
                    'link': link,
                    'published': datetime.now().strftime("%Y-%m-%d"),
                    'source': 'jobkorea',
                    'company_key': company_key,
                })
            except Exception:
                continue

    except Exception:
        pass

    return items


def check_for_new_recruitment():
    """
    ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ìƒˆ ì±„ìš©ê³µê³  í™•ì¸.
    í•˜ë£¨ 1íšŒë§Œ ì‹¤í–‰í•˜ë„ë¡ ì²´í¬í•©ë‹ˆë‹¤.
    Returns: dict with 'IBK' and 'KDB' lists, plus '_new_found' count
    """
    stored = _load_stored_recruitment()

    # í•˜ë£¨ 1íšŒ ì²´í¬ ì œí•œ
    last_checked = stored.get('_last_checked', '')
    today = datetime.now().strftime("%Y-%m-%d")

    if last_checked == today:
        return stored

    new_count = 0

    for company_key in ["IBK", "KDB"]:
        existing_titles = {item['title'] for item in stored.get(company_key, [])}

        # ê° ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
        news_items = fetch_recruitment_from_news(company_key)
        saramin_items = fetch_recruitment_from_saramin(company_key)
        jobkorea_items = fetch_recruitment_from_jobkorea(company_key)

        all_new = news_items + saramin_items + jobkorea_items

        for item in all_new:
            # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ì •ê·œí™”)
            normalized_title = re.sub(r'\[.*?\]\s*', '', item['title']).strip()
            if normalized_title not in existing_titles and item['title'] not in existing_titles:
                stored.setdefault(company_key, []).insert(0, item)
                existing_titles.add(normalized_title)
                existing_titles.add(item['title'])
                new_count += 1

    stored['_last_checked'] = today
    stored['_new_found'] = new_count
    _save_recruitment(stored)

    return stored


def get_all_recruitment_info(company_key):
    """
    í•˜ë“œì½”ë”©ëœ ê³¼ê±° ì±„ìš©ì •ë³´ + ìë™ìˆ˜ì§‘ëœ ìµœì‹  ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ë°˜í™˜.
    Returns: list of recruitment items (ìµœì‹ ìˆœ ì •ë ¬)
    """
    import company_data

    # 1. í•˜ë“œì½”ë”©ëœ ê³¼ê±° ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    company_name = "IBK Capital" if company_key == "IBK" else "KDB Capital"
    static_data = company_data.company_info.get(company_name, {})
    static_recruitment = static_data.get('recruitment', [])

    # 2. ìë™ìˆ˜ì§‘ ë°ì´í„° ë¡œë“œ
    stored = _load_stored_recruitment()
    auto_items = stored.get(company_key, [])

    # 3. ìë™ìˆ˜ì§‘ ë°ì´í„°ë¥¼ í‘œì‹œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    auto_recruitment = []
    for item in auto_items:
        auto_recruitment.append({
            'title': f"ğŸ”„ {item['title']}",
            'period': item.get('published', '')[:10],
            'roles': [],
            'scale': '',
            'note': f"ì¶œì²˜: {item.get('source', 'auto')} | {item.get('conditions', '')}".strip(' |'),
            'link': item.get('link', ''),
            'deadline': item.get('deadline', ''),
            'is_auto': True,
        })

    # 4. í†µí•© (ìë™ìˆ˜ì§‘ ìµœì‹  ë°ì´í„° + í•˜ë“œì½”ë”© ê³¼ê±° ë°ì´í„°)
    return auto_recruitment + static_recruitment
