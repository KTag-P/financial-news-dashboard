
import simple_summarizer
from collections import Counter
import re
from datetime import datetime

def extract_keywords(text, top_n=5):
    """
    Extracts top N keywords from text, excluding common stopwords.
    """
    # Simple stopword list (can be expanded)
    stopwords = set(["ë‰´ìŠ¤", "ê¸°ì", "ë°í˜”ë‹¤", "ë”°ë¥´ë©´", "ìˆë‹¤", "ê²ƒìœ¼ë¡œ", "ëŒ€í•œ", "ìœ„í•´", "í†µí•´", "ì§€ë‚œ", "ì´ë²ˆ", "ê²½ìš°", "ê´€ë ¨", "ë“±", "ë°", "ì´", "ê·¸", "ì €", "ìˆ˜", "ê²ƒ", "ë“¤", "ì œ", "ê°œ", "ì „", "í›„", "ë„¤", "ì•„", "íœ´", "ì•„ì´êµ¬", "ì•„ì´ì¿ ", "ì•„ì´ê³ ", "ì–´", "ë‚˜", "ìš°ë¦¬", "ì €í¬", "ë”°ë¼", "ì˜í•´", "ì„", "ë¥¼", "ì—", "ì˜", "ê°€", "ìœ¼ë¡œ", "ë¡œ", "ì—ê²Œ", "ë¿ì´ë‹¤", "ì˜ê±°í•˜ì—¬", "ê·¼ê±°í•˜ì—¬", "ì…ê°í•˜ì—¬", "ê¸°ì¤€ìœ¼ë¡œ", "ì˜ˆí•˜ë©´", "ì˜ˆë¥¼", "ë“¤ë©´", "ë“¤ìë©´", "ì €ê¸°", "ì €ìª½", "ì €ê²ƒ", "ê·¸ë•Œ", "ê·¸ëŸ¼", "ê·¸ëŸ¬ë©´", "ìš”ì»¨ëŒ€", "ë‹¤ì‹œ", "ë§í•˜ìë©´", "ë§í•˜ë©´", "ì¦‰", "êµ¬ì²´ì ìœ¼ë¡œ", "ë§í•´", "ì‹œì‘í•˜ì—¬", "ê´€í•˜ì—¬", "ë¹„ê¸¸ìˆ˜", "ì—†ë‹¤", "í•˜ê¸°", "ë•Œë¬¸ì—", "ê·¸", "ì—¬ëŸ¬ë¶„"])
    
    words = re.findall(r'\w+', text)
    filtered_words = [w for w in words if len(w) > 1 and w not in stopwords]
    
    # Weight certain financial keywords
    weighted_words = []
    for w in filtered_words:
        weighted_words.append(w)
        if w in ["ê¸ˆë¦¬", "PF", "ë¶€ë™ì‚°", "ì‹¤ì ", "ìˆœì´ìµ", "ë°°ë‹¹", "ì£¼ê°€", "ë°œí–‰", "ì±„ê¶Œ", "CEO", "ì¸ì‚¬", "ë””ì§€í„¸", "í”Œë«í¼"]:
            weighted_words.append(w) # Add again to boost weight
            
    return [item[0] for item in Counter(weighted_words).most_common(top_n)]

def generate_synthesis_report(news_items, title="ì›”ê°„ í•µì‹¬ ë¦¬í¬íŠ¸"):
    """
    Generates a synthesized report from multiple news items.
    """
    now = datetime.now().strftime("%Y-%m-%d")
    
    # 1. Combine All Text
    all_text = ""
    for item in news_items:
        content = item.get('full_content', '') or item.get('summary', '')
        all_text += content + "\n"
    
    if len(all_text) < 100:
        return "ë¶„ì„í•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."

    # 2. Extract Top Themes
    keywords = extract_keywords(all_text, top_n=5)
    
    # 3. Generate Executive Summary (Overall)
    exec_summary = simple_summarizer.summarize_korean(all_text, num_sentences=5)
    
    # 4. Generate Theme-based Sections
    theme_sections = ""
    used_sentences = set() # To avoid repetition
    
    for kw in keywords:
        # Find sentences containing the keyword
        relevant_sentences = []
        for sentence in re.split(r'(?<=[.?!])\s+', all_text):
            if kw in sentence and len(sentence) > 30:
                clean_s = simple_summarizer.clean_text(sentence)
                if clean_s not in used_sentences:
                    relevant_sentences.append(clean_s)
        
        # Summarize these sentences
        if relevant_sentences:
            theme_summary = simple_summarizer.summarize_korean(" ".join(relevant_sentences), num_sentences=2)
            theme_sections += f"#### ğŸ”‘ í‚¤ì›Œë“œ: {kw}\n{theme_summary}\n\n"
            used_sentences.add(theme_summary)

    # 5. Build Final Markdown
    report = f"""
## ğŸ“Š {title} (AI Synthesis)

### ğŸ“ ì¢…í•© ìš”ì•½ (Executive Summary)
{exec_summary}

---

### ğŸ” ì£¼ìš” í‚¤ì›Œë“œë³„ ì‹¬ì¸µ ë¶„ì„
{theme_sections}

---

### ğŸ—ï¸ ë¶„ì„ ëŒ€ìƒ ë‰´ìŠ¤ ({len(news_items)}ê±´)
"""
    for item in news_items[:10]: # List top 10 titles only
        report += f"- {item.get('title')}\n"
        
    return report
